{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Agent Evaluation - Weni Fork","text":"<p>Note: This is a fork of the original Agent Evaluation framework by AWS Labs. This fork adds support for testing Weni conversational AI agents while maintaining all the original functionality for AWS services.</p> <p>Agent Evaluation is a generative AI-powered framework for testing virtual agents.</p> <p>Internally, Agent Evaluation implements an LLM agent (evaluator) that will orchestrate conversations with your own agent (target) and evaluate the responses during the conversation.</p>"},{"location":"#key-features","title":"\u2728 Key features","text":"<ul> <li>\ud83c\udd95 Weni Agent Support: Built-in support for testing Weni conversational AI agents through their API and WebSocket interface.</li> <li>Built-in support for popular AWS services including Amazon Bedrock, Amazon Q Business, and Amazon SageMaker. You can also bring your own agent to test using Agent Evaluation.</li> <li>Orchestrate concurrent, multi-turn conversations with your agent while evaluating its responses.</li> <li>Define hooks to perform additional tasks such as integration testing.</li> <li>Can be incorporated into CI/CD pipelines to expedite the time to delivery while maintaining the stability of agents in production environments.</li> </ul>"},{"location":"#quick-start-with-weni","title":"\ud83d\ude80 Quick Start with Weni","text":""},{"location":"#installation","title":"Installation","text":"<p>Install the package from PyPI:</p> <pre><code>pip install weni-agenteval\n</code></pre>"},{"location":"#prerequisites","title":"Prerequisites","text":"<p>Important</p> <p>You need both AWS and Weni credentials to run evaluations!</p> <p>To test Weni agents, you'll need:</p> <ol> <li>AWS Credentials: Required for the evaluator (Claude model via Bedrock)</li> <li>AWS Access Key ID</li> <li>AWS Secret Access Key  </li> <li> <p>AWS Session Token</p> </li> <li> <p>A Weni Project: An active project in the Weni platform</p> </li> <li> <p>Weni Authentication: Choose one of the following methods:</p> </li> </ol> <p>\ud83d\ude80 Option 1: Weni CLI (Recommended)</p> <p>Install and authenticate with the Weni CLI:    <pre><code># Install Weni CLI\npip install weni-cli\n\n# Authenticate with Weni\nweni login\n\n# Select your project\nweni project use [your-project-uuid]\n</code></pre></p> <p>\ud83d\udccb Option 2: Environment Variables</p> <p>Set these environment variables manually:    - <code>WENI_PROJECT_UUID</code>: Your project's unique identifier    - <code>WENI_BEARER_TOKEN</code>: Your authentication bearer token</p>"},{"location":"#basic-usage","title":"Basic Usage","text":"<p>Create a test configuration file <code>agenteval.yml</code>:</p> <pre><code>evaluator:\n  model: claude-haiku-4_5-global  # or claude-sonnet-4_5-global, claude-haiku-3_5-us\n  aws_region: us-east-1\n\ntarget:\n  type: weni\n\ntests:\n  greeting:\n    steps:\n      - Send a greeting \"Ol\u00e1, bom dia!\"\n      - Ask what \"com oq vc pode me ajudar?\"\n    expected_results:\n      - Agent responds with a friendly greeting\n      - Agent shows up a menu with options to help the user\n\n  purchase_outside_postal_code:\n    steps:\n      - Ask information \"quero comprar arroz\"\n      - Give the postal code \"04538-132\"\n    expected_results:\n      - Agent responds asking for postal code\n      - Agent says it doesn't deliver to this postal code\n</code></pre> <p>Run the evaluation:</p> <pre><code>weni-agenteval run\n</code></pre> <p>For real-time monitoring of conversations, use watch mode:</p> <pre><code>weni-agenteval run --watch\n</code></pre> <ul> <li> <p>\ud83d\ude80 Getting started</p> <p>Create your first Weni agent test.</p> <p> User Guide</p> </li> <li> <p>\ud83c\udfaf Weni Target Configuration</p> <p>Learn how to configure your Weni agent for testing.</p> <p> Weni Target</p> </li> <li> <p>\u270f\ufe0f Writing test cases</p> <p>Learn how to write effective test cases for conversational AI.</p> <p> User Guide</p> </li> <li> <p> Contribute</p> <p>Review the contributing guidelines to get started!</p> <p> GitHub</p> </li> </ul>"},{"location":"cli/","title":"agenteval","text":"<p>Usage:</p> <pre><code>agenteval [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <pre><code>  --help  Show this message and exit.\n</code></pre>"},{"location":"cli/#agenteval-init","title":"agenteval init","text":"<p>Initialize a test plan.</p> <p>Usage:</p> <pre><code>agenteval init [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  --plan-dir TEXT  The directory to store the test plan. If a directory is not\n                   provided, the test plan will be saved to the current\n                   working directory.\n  --help           Show this message and exit.\n</code></pre>"},{"location":"cli/#agenteval-run","title":"agenteval run","text":"<p>Run test plan.</p> <p>Usage:</p> <pre><code>agenteval run [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  --filter TEXT          Specifies the test(s) to run, where multiple tests\n                         should be seperated using a comma. If a filter is not\n                         provided, all tests will be run.\n  --plan-dir TEXT        The directory where the test plan is stored. If a\n                         directory is not provided, the test plan will be read\n                         from the current working directory.\n  --verbose              Whether to enable verbose logging. Defaults to False.\n  --num-threads INTEGER  Number of threads used to run tests concurrently. If\n                         the number of threads is not provided, the thread\n                         count will be set to the number of tests (up to a\n                         maximum of 45 threads).\n  --work-dir TEXT        The directory where the test result and trace will be\n                         generated. If a directory is not provided, the assets\n                         will be saved to the current working directory.\n  --watch                Enable watch mode to see real-time user and agent\n                         interactions during test execution. Tests will run\n                         sequentially for better readability.\n  --help                 Show this message and exit.\n</code></pre>"},{"location":"configuration/","title":"Configuration","text":"agenteval.yml<pre><code>evaluator:\n  model: claude-3\ntarget:\n  type: bedrock-agent\n  bedrock_agent_id: string\n  bedrock_agent_alias_id: string\ntests:\n  retrieve_missing_documents:\n    steps:\n    - Ask agent for a list of missing documents for claim-006.\n    expected_results:\n    - The agent returns a list of missing documents.\n    initial_prompt: Give me a list of missing documents for claim-006.\n    max_turns: 2\n    hook: path.to.MyHook\n</code></pre> <p><code>evaluator</code> (map)</p> <p>Refer to Evaluators for the available configurations.</p> <p><code>target</code> (map)</p> <p>Refer to Targets for the available configurations.</p> <p><code>tests</code> (map)</p> <p>A map of test cases, where the test name serves as the key.</p> <p><code>steps</code> (list of strings)</p> <p>The steps to perform for the test.</p> <p><code>expected_results</code> (list of strings)</p> <p>The expected results for the test.</p> <p><code>initial_prompt</code> (string; optional)</p> <p>The first message that is sent to the agent, which starts the conversation. If unspecified, the message will be generated based on the <code>steps</code> provided.</p> <p><code>max_turns</code> (integer; optional)</p> <p>The maximum number of user-agent exchanges before the test fails. The default is <code>2</code>.</p> <p><code>hook</code> (string; optional)</p> <p>The module path to an evaluation hook. Refer to Hooks for more details.</p>"},{"location":"hooks/","title":"Hooks","text":"<p>You can specify hooks that run before and/or after evaluating a test. This is useful for performing integration testing, as well as any setup or cleanup tasks required.</p> <p>To create your hooks, define a Python module containing a subclass of Hook. The name of this module must contain the suffix <code>_hook</code> (e.g. <code>my_evaluation_hook</code>).</p> <ul> <li> <p>Implement the <code>pre_evaluate</code> method for a hook that runs before evaluation. In this method, you have access to the Test and Trace via the <code>test</code> and <code>trace</code> arguments, respectively.</p> </li> <li> <p>Implement the <code>post_evaluate</code> method for a hook that runs after evaluation,. Similar to the <code>pre_evaluate</code> method, you have access to the Test and Trace. You also have access to the TestResult via the <code>test_result</code> argument. You may override the attributes of the <code>TestResult</code> if you plan to use this hook to perform additional testing, such as integration testing.</p> </li> </ul> my_evaluation_hook.py<pre><code>from agenteval import Hook\n# import dependencies here\n\nclass MyEvaluationHook(Hook):\n\n    def pre_evaluate(test, trace):\n    # implement logic here\n\n    def post_evaluate(test, test_result, trace):\n    # implement logic here\n</code></pre> <p>Once you have created your subclass, specify the module path to the hook.</p> agenteval.yml<pre><code>tests:\n  make_reservation:\n    hook: my_evaluation_hook.MyEvaluationHook\n</code></pre>"},{"location":"hooks/#examples","title":"Examples","text":""},{"location":"hooks/#integration-testing-using-post_evaluate","title":"Integration testing using <code>post_evaluate</code>","text":"<p>In this example, we will test an agent that can make dinner reservations. In addition to evaluating the conversation, we want to test that the reservation is written to the backend database. To do this, we will create a post evaluation hook that queries a PostgreSQL database for the reservation record. If the record is not found, we will override the <code>TestResult</code>.</p> <p>test_record_insert_hook.py</p> <pre><code>import boto3\nimport json\nimport psycopg2\n\nfrom agenteval import Hook\n\nSECRET_NAME = \"test-secret\"\n\ndef get_db_secret() -&gt; dict:\n\n  session = boto3.session.Session()\n  client = session.client(\n      service_name='secretsmanager',\n  )\n  get_secret_value_response = client.get_secret_value(\n      SecretId=SECRET_NAME\n  )\n  secret = get_secret_value_response['SecretString']\n\n  return json.loads(secret)\n\nclass TestRecordInsertHook(Hook):\n\n  def post_evaluate(test, test_result, trace):\n\n    # get database secret from AWS Secrets Manager\n    secret = get_db_secret()\n\n    # connect to database\n    conn = psycopg2.connect(\n      database=secret[\"dbname\"],\n      user=secret[\"username\"],\n      password=secret[\"password\"],\n      host=secret[\"host\"],\n      port=secret[\"port\"]\n    )\n\n    # check if record is written to database\n    with conn.cursor() as cur:\n      cur.execute(\"SELECT * FROM reservations WHERE name = 'Bob'\")\n      row = cur.fetchone()\n\n    # override the test result based on query result \n    if not row:\n      test_result.passed = False\n      test_result.result = \"Integration test failed\"\n      test_result.reasoning = \"Record was not inserted into the database\"\n</code></pre> <p>Create a test that references the hook.</p> <p>agenteval.yml</p> <pre><code>tests:\n  make_reservation:\n    steps:\n    - Ask agent to make a reservation under the name Bob for 7 PM.\n    expected_results:\n    - The agent confirms that a reservation has been made.\n    hook: test_record_insert_hook.TestRecordInsert\n</code></pre>"},{"location":"installation/","title":"Installation","text":"<p>Agent Evaluation requires <code>python&gt;=3.9</code>. Please make sure you have an acceptable version of Python before proceeding.</p>"},{"location":"installation/#install-from-pypi-recommended","title":"Install from PyPI (Recommended)","text":"<pre><code>pip install weni-agenteval\n</code></pre>"},{"location":"installation/#install-from-source","title":"Install from Source","text":"<p>You can also install from source by cloning the repository and installing from the project root.</p> <pre><code>git clone https://github.com/weni-ai/agent-evaluation.git\ncd agent-evaluation\npip install -e .\n</code></pre>"},{"location":"installation/#prerequisites-for-weni-target","title":"Prerequisites for Weni Target","text":"<p>Important</p> <p>You need both AWS and Weni credentials to run evaluations!</p>"},{"location":"installation/#aws-credentials-required-for-evaluator","title":"AWS Credentials (Required for Evaluator)","text":"<p>The evaluator uses Amazon Bedrock's Claude models, so you'll need:</p> <ul> <li>AWS Access Key ID</li> <li>AWS Secret Access Key</li> <li>AWS Session Token</li> </ul> <p>Set these as environment variables:</p> <p>macOS/Linux: <pre><code>export AWS_ACCESS_KEY_ID=\"your-aws-access-key-id\"\nexport AWS_SECRET_ACCESS_KEY=\"your-aws-secret-access-key\"\nexport AWS_SESSION_TOKEN=\"your-aws-session-token\"\n</code></pre></p> <p>Windows (Command Prompt): <pre><code>set AWS_ACCESS_KEY_ID=your-aws-access-key-id\nset AWS_SECRET_ACCESS_KEY=your-aws-secret-access-key\nset AWS_SESSION_TOKEN=your-aws-session-token\n</code></pre></p> <p>Windows (PowerShell): <pre><code>$env:AWS_ACCESS_KEY_ID=\"your-aws-access-key-id\"\n$env:AWS_SECRET_ACCESS_KEY=\"your-aws-secret-access-key\"\n$env:AWS_SESSION_TOKEN=\"your-aws-session-token\"\n</code></pre></p>"},{"location":"installation/#weni-authentication","title":"Weni Authentication","text":"<p>Choose one of the following authentication methods:</p>"},{"location":"installation/#option-1-weni-cli-recommended","title":"Option 1: Weni CLI (Recommended)","text":"<p>Install and authenticate with the Weni CLI:</p> <pre><code># Install Weni CLI\npip install weni-cli\n\n# Authenticate with Weni\nweni login\n\n# Select your project\nweni project use [your-project-uuid]\n</code></pre> <p>Get the Weni CLI from: https://github.com/weni-ai/weni-cli</p>"},{"location":"installation/#option-2-environment-variables","title":"Option 2: Environment Variables","text":"<p>If you prefer not to use the Weni CLI, set these environment variables:</p> <p>macOS/Linux: <pre><code>export WENI_PROJECT_UUID=\"your-project-uuid-here\"\nexport WENI_BEARER_TOKEN=\"your-bearer-token-here\"\n</code></pre></p> <p>Windows (Command Prompt): <pre><code>set WENI_PROJECT_UUID=your-project-uuid-here\nset WENI_BEARER_TOKEN=your-bearer-token-here\n</code></pre></p> <p>Windows (PowerShell): <pre><code>$env:WENI_PROJECT_UUID=\"your-project-uuid-here\"\n$env:WENI_BEARER_TOKEN=\"your-bearer-token-here\"\n</code></pre></p>"},{"location":"installation/#option-3-configuration-file","title":"Option 3: Configuration File","text":"<p>You can also provide credentials directly in your test configuration file (not recommended for production):</p> <pre><code>target:\n  type: weni\n  weni_project_uuid: your-project-uuid-here\n  weni_bearer_token: your-bearer-token-here\n</code></pre>"},{"location":"installation/#verify-installation","title":"Verify Installation","text":"<p>After installation, verify that everything works:</p> <pre><code>weni-agenteval --help\n</code></pre> <p>You should see the help message with available commands and options.</p>"},{"location":"user_guide/","title":"User Guide","text":""},{"location":"user_guide/#getting-started","title":"Getting started","text":"<p>To begin, initialize a test plan for your Weni agent.</p> <pre><code>weni-agenteval init\n</code></pre> <p>This will create a configuration file named <code>agenteval.yml</code> in the current directory.</p> agenteval.yml<pre><code>evaluator:\n  model: claude-haiku-4_5-global  # or claude-sonnet-4_5-global for higher accuracy\n  aws_region: us-east-1\ntarget:\n  type: weni\ntests:\n  greeting:\n    steps:\n    - Send a greeting \"Ol\u00e1, bom dia!\"\n    expected_results:\n    - Agent responds with a friendly greeting\n</code></pre> <p>Update the <code>target</code> configuration for your Weni agent:</p> <ul> <li><code>type</code>: Must be <code>\"weni\"</code> for Weni agents</li> <li><code>timeout</code>: Optional timeout in seconds (defaults to 30)</li> </ul> <p>Note</p> <p>You need both AWS credentials (for the evaluator) and Weni authentication. See Installation for setup instructions.</p> <p>Update <code>tests</code> with your test cases. Each test must have the following:</p> <ul> <li><code>steps</code>: A list of steps you want to perform in your test.</li> <li><code>expected_results</code>: A list of expected results for your test.</li> </ul> <p>Once you have updated the test plan, you can run your tests:</p> <p>Warning</p> <p>The default evaluator is powered by Anthropic's Claude model on Amazon Bedrock. The charges you incur from using Amazon Bedrock will be your responsibility. Please review this page on evaluator costs before running your tests.</p> <pre><code>weni-agenteval run\n</code></pre> <p>The results will be printed in your terminal and a Markdown summary will be available in <code>agenteval_summary.md</code>.</p> <p>You will also find traces saved under <code>agenteval_traces/</code>. This is useful for understanding the flow of evaluation.</p>"},{"location":"user_guide/#watch-mode","title":"Watch Mode","text":"<p>For real-time monitoring of your tests, you can use the <code>--watch</code> mode to see user and agent interactions as they happen:</p> <pre><code>weni-agenteval run --watch\n</code></pre> <p>Watch mode provides:</p> <ul> <li>Real-time conversation display: See user messages and agent responses as they occur</li> <li>Immediate feedback: User prompts appear instantly when sent to the agent</li> <li>Visual test results: Clear \u2705 PASS / \u274c FAIL indicators for each test</li> <li>Sequential execution: Tests run one at a time for readable output</li> <li>Progress tracking: Shows current test progress and overall completion</li> </ul>"},{"location":"user_guide/#watch-mode-output","title":"Watch Mode Output","text":"<p>When using watch mode, you'll see output like this:</p> <pre><code>================================================================================\n\ud83d\udd0d WATCH MODE: Running 2 test(s) sequentially\n================================================================================\n\n\ud83d\udccb Test 1/2: greeting\n------------------------------------------------------------\n\n\ud83d\udc64 USER: Ol\u00e1, bom dia!\n\ud83e\udd16 AGENT: Ol\u00e1! Bom dia! Como posso ajud\u00e1-lo hoje?\n\n\ud83d\udc64 USER: com oq vc pode me ajudar?\n\ud83e\udd16 AGENT: Posso ajud\u00e1-lo com:\n\u2022 Informa\u00e7\u00f5es sobre produtos\n\u2022 Rastreamento de pedidos\n\u2022 Suporte t\u00e9cnico\n\u2022 Consultas gerais\n\n\u2705 PASSED: greeting\n   Result: All expected results can be observed in the conversation.\n   Reasoning: The agent provided appropriate responses to all user inputs.\n\n================================================================================\n\n\ud83d\udccb Test 2/2: purchase_outside_postal_code\n------------------------------------------------------------\n[... more conversation ...]\n\n\ud83c\udfc1 WATCH MODE COMPLETED: 2/2 tests passed\n================================================================================\n</code></pre>"},{"location":"user_guide/#watch-mode-options","title":"Watch Mode Options","text":"<p>You can combine watch mode with other CLI options:</p> <pre><code># Run specific tests in watch mode\nweni-agenteval run --watch --filter greeting,purchase_test\n\n# Use watch mode with a specific plan directory\nweni-agenteval run --watch --plan-dir /path/to/your/tests\n\n# Run watch mode with verbose logging\nweni-agenteval run --watch --verbose\n</code></pre> <p>When to Use Watch Mode</p> <p>Watch mode is particularly useful for:</p> <ul> <li>Development: Debugging test cases and understanding agent behavior</li> <li>Demonstrations: Showing stakeholders how tests interact with agents</li> <li>Learning: Understanding the conversation flow and evaluation process</li> <li>Troubleshooting: Identifying where conversations go wrong</li> </ul> <p>Performance Considerations</p> <p>Watch mode runs tests sequentially (one at a time) to ensure readable output. For faster execution of many tests, use the regular mode without <code>--watch</code>.</p>"},{"location":"user_guide/#writing-test-cases","title":"Writing test cases","text":"<p>It is important to be clear and concise when writing your test cases for conversational AI agents.</p> agenteval.yml<pre><code>tests:\n  product_inquiry:\n    steps:\n    - Ask \"Quais produtos voc\u00eas t\u00eam dispon\u00edveis?\"\n    expected_results:\n    - Agent provides information about available products\n    - Response includes clear product descriptions or categories\n</code></pre> <p>If your test case is complex, consider breaking it down into multiple, smaller <code>tests</code>.</p>"},{"location":"user_guide/#multi-turn-conversations","title":"Multi-turn conversations","text":"<p>To test multiple user-agent interactions, you can provide multiple <code>steps</code> to orchestrate the interaction.</p> agenteval.yml<pre><code>tests:\n  product_and_delivery:\n    steps:\n    - Ask \"Quais produtos voc\u00eas t\u00eam?\"\n    - Ask \"Qual \u00e9 o prazo de entrega?\"\n    expected_results:\n    - Agent provides product information\n    - Agent maintains context and provides delivery timeframe\n</code></pre> <p>The maximum number of turns allowed for a conversation is configured using the <code>max_turns</code> parameter for the test (defaults to <code>2</code> when not specified). If the number of turns in the conversation reaches the <code>max_turns</code> limit, then the test will fail.</p>"},{"location":"user_guide/#providing-data","title":"Providing data","text":"<p>You can test an agent's ability to prompt the user for data when you include it within the step. For example:</p> agenteval.yml<pre><code>tests:\n  purchase_with_postal_code:\n    steps:\n    - Ask \"Quero comprar arroz\".\n      When the agent asks for postal code, respond with \"01310-100\".\n    expected_results:\n    - Agent confirms the product selection\n    - Agent processes the postal code and confirms delivery availability\n</code></pre>"},{"location":"user_guide/#specify-the-first-user-message","title":"Specify the first user message","text":"<p>By default, the first user message in the test is automatically generated based on the first step. To override this message, you can specify the <code>initial_prompt</code>.</p> agenteval.yml<pre><code>tests:\n  business_hours_inquiry:\n    steps:\n    - Ask about business hours and weekend availability.\n    initial_prompt: Qual \u00e9 o hor\u00e1rio de funcionamento da loja?\n    expected_results:\n    - Agent provides clear business hours information\n    - Agent includes both weekday and weekend hours\n</code></pre>"},{"location":"cicd/","title":"Integration with CI/CD Pipelines","text":"<p>After validating the functionality in the development account, you can commit the code to the repository and initiate the deployment process for the virtual agent to the next stage. Seamless integration with CI/CD pipelines is a crucial aspect of Agent Evaluation, enabling comprehensive integration testing to ensure that no regressions are introduced during new feature development or updates. This rigorous testing approach is vital for maintaining the reliability and consistency of virtual agents as they progress through the software delivery lifecycle.</p> <p>By incorporating Agent Evaluation into CI/CD workflows, organizations can automate the testing process, ensuring that every code change or update undergoes thorough evaluation before deployment. This proactive measure minimizes the risk of introducing bugs or inconsistencies that could compromise the virtual agent's performance and the overall user experience.</p>"},{"location":"cicd/#cicd-workflow","title":"CI/CD workflow","text":"<p>The figure below shows what a standard agent CI/CD pipeline looks like:</p> <p></p> <ol> <li>The source repository stores the agent configuration, including agent instructions, system prompts, model configuration, etc. You should always commit your changes to ensure quality and reproducibility.</li> <li>When you commit your changes, a build step is triggered. This is where unit tests should run and validate the changes, including typo and syntax checks.</li> <li>When the changes are deployed to the staging environment, Agent Evaluation should run with a series of test cases for runtime validation.</li> <li>The runtime validation on the staging environment will help build confidence to deploy the fully tested agent to production.</li> </ol>"},{"location":"cicd/#step-by-step-github-actions-setup","title":"Step-by-step GitHub Actions setup","text":"<p>We have built an example with GitHub Actions, please take a look at the Github workflow. Here is the step-by-step setup guide:</p> <ol> <li> <p>Write a series of test cases following the agent-evaluation test plan syntax. Store test plans in the git repository. For example, a test plan to test a Bedrock agent target is written as follows, with <code>BEDROCK_AGENT_ALIAS_ID</code> and <code>BEDROCK_AGENT_ID</code> as placeholders:</p> <pre><code>evaluator:\nmodel: claude-3\ntarget:\n    bedrock_agent_alias_id: BEDROCK_AGENT_ALIAS_ID\n    bedrock_agent_id: BEDROCK_AGENT_ID\ntype: bedrock-agent\ntests:\n    InsuranceClaimQuestions:\n    ...\n</code></pre> </li> <li> <p>Create an IAM user with proper permissions:</p> <ol> <li>The principal must have <code>InvokeModel</code> permission to the model specified in the configuration.</li> <li>The principal must have the permissions to call the target agent. Depending on the target type, different permissions are required. Please visit the agent-evaluation Target docs for details.</li> </ol> </li> <li>Store the IAM credentials (<code>AWS_ACCESS_KEY_ID</code> and <code>AWS_SECRET_ACCESS_KEY</code>  ) in GitHub Actions secrets. </li> <li>Configure a GitHub workflow  as follows:     <pre><code>name: CI/CD example\n\non:\npush:\n    branches: [ \"main\" ]\n\nenv:\nAWS_REGION: us-east-1                   # set this to your preferred AWS region, e.g. us-west-1\n\n\npermissions:\ncontents: read\n\njobs:\ndeploy:\n    runs-on: ubuntu-latest\n\n    steps:\n    - name: Checkout\n    uses: actions/checkout@v4\n\n    - name: Configure AWS credentials\n    uses: aws-actions/configure-aws-credentials@v4\n    with:\n        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}\n        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}\n        aws-region: ${{ env.AWS_REGION }}\n\n    - name: Install weni-agenteval\n    run: |\n        pip install weni-agenteval\n        weni-agenteval --help\n    - name: Test Bedrock Agent\n    id: test-bedrock-agent\n    env:\n        BEDROCK_AGENT_ALIAS_ID: ${{ vars.BEDROCK_AGENT_ALIAS_ID }}\n        BEDROCK_AGENT_ID: ${{ vars.BEDROCK_AGENT_ID }}\n    run: |\n        sed -e \"s/BEDROCK_AGENT_ALIAS_ID/$BEDROCK_AGENT_ALIAS_ID/g\" -e \"s/BEDROCK_AGENT_ID/$BEDROCK_AGENT_ID/g\" &lt;path-to-the-test-plan-template-file&gt; &gt; agenteval.yml\n        weni-agenteval run\n    - name: Test Summary\n    if: always()\n    id: test-summary\n    run: |\n        if [ -f agenteval_summary.md ]; then\n        cat agenteval_summary.md &gt;&gt; $GITHUB_STEP_SUMMARY\n        fi\n</code></pre></li> </ol>"},{"location":"evaluators/","title":"Evaluators","text":"<p>An evaluator is a LLM agent that evaluates a Target on a test. Evaluators utilize foundation models directly on Amazon Bedrock. They do not make use of the Agents for Amazon Bedrock functionality.</p>"},{"location":"evaluators/#evaluation-workflow","title":"Evaluation workflow","text":"<p>The diagram below depicts the workflow that is conducted during evaluation.</p> <pre><code>graph TD\n  classDef nodeText font-size:10pt;\n  A((Start)) --&gt; B{Initial&lt;br&gt;prompt?}\n  B --&gt;|yes| C(Invoke agent)\n  B --&gt;|no| D(Generate initial prompt)\n  D --&gt; C\n  C --&gt; E(Get test status)\n  E --&gt; F{All steps&lt;br&gt;attempted?}  \n  F --&gt; |yes| G(Evaluate conversation)\n  F --&gt; |no| H{Max turns&lt;br&gt;reached?}\n  H --&gt; |yes| I(Fail)\n  H --&gt; |no| J(Generate user response)\n  J --&gt; C\n  G --&gt; K{All expected&lt;br&gt;results&lt;br&gt;observed?}\n  K --&gt; |yes| L(Pass)\n  K --&gt; |no| I(Fail)\n  I --&gt; M((End))\n  L --&gt; M\n  class A,B,C,D,E,F,G,H,I,J,K,L,M nodeText;\n  style I stroke:#f00\n  style L stroke:#0f0</code></pre>"},{"location":"evaluators/#evaluator-costs","title":"Evaluator costs","text":"<p>By default, evaluators will utilize the InvokeModel API with On-Demand mode, which will incur AWS charges based on input tokens processed and output tokens generated. You can find the latest pricing details for Amazon Bedrock here.</p> <p>The cost of running an evaluator for a single test is influenced by the following:</p> <ol> <li>The number and length of the steps.</li> <li>The number and length of expected results.</li> <li>The length of the target agent's responses.</li> </ol> <p>You can view the total number of input tokens processed and output tokens generated by the evaluator using <code>--verbose</code> flag when you perform a run (<code>agenteval run --verbose</code>).</p> <p>Note</p> <p>If you have purchased Provisioned Throughput model units for the <code>model</code> used to run evaluation, you can specify this resource using the <code>provisioned_throughput_arn</code> configuration.</p>"},{"location":"evaluators/#example","title":"Example","text":"<p>Let's use this Amazon Bedrock agent as a target we want to test.</p> <p>For the following test case:</p> agenteval.yml<pre><code>tests:\n  retrieve_missing_documents:\n    steps:\n    - Ask agent for a list of missing documents for claim-006.\n    expected_results:\n    - The agent returns a list of missing documents.\n</code></pre> <p>We find that on average, the evaluator processes ~583 input tokens and generates ~290 output tokens. </p>"},{"location":"evaluators/#prerequisites","title":"Prerequisites","text":"<p>The principal must have InvokeModel to the <code>model</code> specified in the configuration.</p>"},{"location":"evaluators/#configurations","title":"Configurations","text":"<p>Info</p> <p>This project uses Boto3's credential resolution chain to determine the AWS credentials to use. Please refer to the Boto3 documentation for more details.</p> agenteval.yml<pre><code>evaluator:\n  model: claude-3\n  provisioned_throughput_arn: my-throughput-arn\n  aws_profile: my-profile\n  aws_region: us-west-2\n  endpoint_url: my-endpoint-url\n  max_retry: 10\n</code></pre> <p><code>model</code> (string)</p> <p>Name of the model used to run evaluation. This must be one of:</p> <ul> <li><code>claude-3</code> (Claude 3 Sonnet)</li> <li><code>claude-3_5</code> (Claude 3.5 Sonnet)</li> <li><code>claude-3_7-us</code> (Claude 3.7 Sonnet)</li> <li><code>claude-haiku-3_5-us</code> (Claude 3.5 Haiku)</li> <li><code>claude-sonnet-4_5-global</code> (Claude Sonnet 4.5)</li> <li><code>claude-haiku-4_5-global</code> (Claude Haiku 4.5)</li> <li><code>llama-3_3-us</code> (Llama 3.3 70B)</li> </ul>"},{"location":"evaluators/#models-suffixed-with-us-use-the-default-usa-cross-region-inference-profile-models-suffixed-with-global-use-the-global-inference-profile-for-worldwide-availability-see-bedrock-cross-region-documentation","title":"Models suffixed with <code>-us</code> use the default USA cross-region inference profile. Models suffixed with <code>-global</code> use the global inference profile for worldwide availability. See Bedrock cross-region documentation.","text":"<p><code>custom-config</code> (dict; optional)</p> <p>A valid combination with keys <code>model_id</code> and <code>request_body</code> specifying which foundation model with what configuration to invoke Bedrock. See Bedrock documentation or the default configurations in <code>src/agenteval/evaluators/model_config/preconfigured_model_configs.py</code>. Currently, only Meta and Anthropic models are supported.</p> <p><code>provisioned_throughput_arn</code> (string; optional)</p> <p>The Amazon Resource Name (ARN) of the Provisioned Throughput.</p> <p><code>aws_profile</code> (string; optional)</p> <p>A profile name that is used to create a Boto3 session.</p> <p><code>aws_region</code> (string; optional)</p> <p>The AWS region that is used to create a Boto3 session.</p> <p><code>endpoint_url</code> (string; optional)</p> <p>The endpoint URL for the AWS service which is used to construct the Boto3 client.</p> <p><code>max_retry</code> (integer; optional)</p> <p>Configures the Boto3 client with the maximum number of retry attempts allowed. The default is <code>10</code>.</p>"},{"location":"reference/base_target/","title":"BaseTarget","text":""},{"location":"reference/base_target/#src.agenteval.targets.base_target.BaseTarget","title":"<code>BaseTarget</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Defines the common interface for target classes.</p> Source code in <code>src/agenteval/targets/base_target.py</code> <pre><code>class BaseTarget(ABC):\n    \"\"\"Defines the common interface for target classes.\"\"\"\n\n    @abstractmethod\n    def invoke(self, prompt: str) -&gt; TargetResponse:\n        \"\"\"Invoke the target with a prompt.\n\n        Args:\n            prompt (str): The prompt as a string.\n\n        Returns:\n            TargetResponse\n        \"\"\"\n        pass\n</code></pre>"},{"location":"reference/base_target/#src.agenteval.targets.base_target.BaseTarget.invoke","title":"<code>invoke(prompt)</code>  <code>abstractmethod</code>","text":"<p>Invoke the target with a prompt.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The prompt as a string.</p> required <p>Returns:</p> Type Description <code>TargetResponse</code> <p>TargetResponse</p> Source code in <code>src/agenteval/targets/base_target.py</code> <pre><code>@abstractmethod\ndef invoke(self, prompt: str) -&gt; TargetResponse:\n    \"\"\"Invoke the target with a prompt.\n\n    Args:\n        prompt (str): The prompt as a string.\n\n    Returns:\n        TargetResponse\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/hook/","title":"Hook","text":""},{"location":"reference/hook/#src.agenteval.hook.Hook","title":"<code>Hook</code>","text":"<p>An evaluation hook.</p> Source code in <code>src/agenteval/hook.py</code> <pre><code>class Hook:\n    \"\"\"An evaluation hook.\"\"\"\n\n    def pre_evaluate(test: Test, trace: Trace) -&gt; None:\n        \"\"\"\n        Method called before evaluation. Can be used to perform any setup tasks.\n\n        Args:\n            test (Test): The test case.\n            trace (Trace): Captures steps during evaluation.\n        \"\"\"\n        pass\n\n    def post_evaluate(test: Test, test_result: TestResult, trace: Trace) -&gt; None:\n        \"\"\"\n        Method called after evaluation. This may be used to perform integration testing\n        or clean up tasks.\n\n        Args:\n            test (Test): The test case.\n            test_result (TestResult): The result of the test, which can be overriden\n                by updating the attributes of this object.\n            trace (Trace): Captures steps during evaluation.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"reference/hook/#src.agenteval.hook.Hook.post_evaluate","title":"<code>post_evaluate(test, test_result, trace)</code>","text":"<p>Method called after evaluation. This may be used to perform integration testing or clean up tasks.</p> <p>Parameters:</p> Name Type Description Default <code>test</code> <code>Test</code> <p>The test case.</p> required <code>test_result</code> <code>TestResult</code> <p>The result of the test, which can be overriden by updating the attributes of this object.</p> required <code>trace</code> <code>Trace</code> <p>Captures steps during evaluation.</p> required Source code in <code>src/agenteval/hook.py</code> <pre><code>def post_evaluate(test: Test, test_result: TestResult, trace: Trace) -&gt; None:\n    \"\"\"\n    Method called after evaluation. This may be used to perform integration testing\n    or clean up tasks.\n\n    Args:\n        test (Test): The test case.\n        test_result (TestResult): The result of the test, which can be overriden\n            by updating the attributes of this object.\n        trace (Trace): Captures steps during evaluation.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/hook/#src.agenteval.hook.Hook.pre_evaluate","title":"<code>pre_evaluate(test, trace)</code>","text":"<p>Method called before evaluation. Can be used to perform any setup tasks.</p> <p>Parameters:</p> Name Type Description Default <code>test</code> <code>Test</code> <p>The test case.</p> required <code>trace</code> <code>Trace</code> <p>Captures steps during evaluation.</p> required Source code in <code>src/agenteval/hook.py</code> <pre><code>def pre_evaluate(test: Test, trace: Trace) -&gt; None:\n    \"\"\"\n    Method called before evaluation. Can be used to perform any setup tasks.\n\n    Args:\n        test (Test): The test case.\n        trace (Trace): Captures steps during evaluation.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/target_response/","title":"TargetResponse","text":""},{"location":"reference/target_response/#src.agenteval.targets.target_response.TargetResponse","title":"<code>TargetResponse</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A target's response.</p> <p>Attributes:</p> Name Type Description <code>response</code> <code>str</code> <p>The response string.</p> <code>data</code> <code>Optional[dict]</code> <p>Additional data (if applicable).</p> Source code in <code>src/agenteval/targets/target_response.py</code> <pre><code>class TargetResponse(BaseModel):\n    \"\"\"A target's response.\n\n    Attributes:\n        response: The response string.\n        data: Additional data (if applicable).\n    \"\"\"\n\n    response: str\n    data: Optional[dict] = None\n</code></pre>"},{"location":"reference/test/","title":"Test","text":""},{"location":"reference/test/#src.agenteval.test.test.Test","title":"<code>Test</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A test case.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Name of the test.</p> <code>steps</code> <code>list[str]</code> <p>List of step to perform for the test.</p> <code>expected_results</code> <code>list[str]</code> <p>List of expected results for the test.</p> <code>initial_prompt</code> <code>Optional[str]</code> <p>The initial prompt.</p> <code>max_turns</code> <code>int</code> <p>Maximum number of turns allowed for the test.</p> <code>hook</code> <code>Optional[str]</code> <p>The module path to an evaluation hook.</p> Source code in <code>src/agenteval/test/test.py</code> <pre><code>class Test(BaseModel, validate_assignment=True):\n    \"\"\"A test case.\n\n    Attributes:\n        name: Name of the test.\n        steps: List of step to perform for the test.\n        expected_results: List of expected results for the test.\n        initial_prompt: The initial prompt.\n        max_turns: Maximum number of turns allowed for the test.\n        hook: The module path to an evaluation hook.\n    \"\"\"\n\n    # do not collect as a pytest\n    __test__ = False\n\n    name: str\n    steps: list[str]\n    expected_results: list[str]\n    initial_prompt: Optional[str] = None\n    max_turns: int\n    hook: Optional[str] = None\n</code></pre>"},{"location":"reference/test_result/","title":"TestResult","text":""},{"location":"reference/test_result/#src.agenteval.test.test_result.TestResult","title":"<code>TestResult</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>The test result.</p> <p>Attributes:</p> Name Type Description <code>test_name</code> <code>str</code> <p>Name of the test.</p> <code>result</code> <code>str</code> <p>Description of the test result.</p> <code>reasoning</code> <code>str</code> <p>The rationale for the test result.</p> <code>passed</code> <code>bool</code> <p><code>True</code> if the test passed, otherwise <code>False</code>.</p> <code>conversation</code> <code>Conversation</code> <p>Captures the interaction between a user and an agent.</p> Source code in <code>src/agenteval/test/test_result.py</code> <pre><code>class TestResult(BaseModel, arbitrary_types_allowed=True):\n    \"\"\"The test result.\n\n    Attributes:\n        test_name: Name of the test.\n        result: Description of the test result.\n        reasoning: The rationale for the test result.\n        passed: `True` if the test passed, otherwise `False`.\n        conversation: Captures the interaction between a user and an agent.\n    \"\"\"\n\n    # do not collect as a pytest\n    __test__ = False\n\n    test_name: str\n    result: str\n    reasoning: str\n    passed: bool\n    conversation: Conversation\n</code></pre>"},{"location":"reference/trace/","title":"Trace","text":""},{"location":"reference/trace/#src.agenteval.trace.Trace","title":"<code>Trace</code>","text":"<p>A context manager which captures steps taken during evaluation.</p> <p>Once the context manager exits, the trace is dumped to a JSON file.</p> <p>Attributes:</p> Name Type Description <code>test_name</code> <code>str</code> <p>Name of the test.</p> <code>trace_dir</code> <code>str</code> <p>Directory to store the trace.</p> <code>start_time</code> <code>datetime</code> <p>Start time of the trace.</p> <code>end_time</code> <code>datetime</code> <p>End time of the trace.</p> <code>steps</code> <code>list</code> <p>List of steps in the trace.</p> Source code in <code>src/agenteval/trace.py</code> <pre><code>class Trace:\n    \"\"\"A context manager which captures steps taken during evaluation.\n\n    Once the context manager exits, the trace is dumped to a JSON file.\n\n    Attributes:\n        test_name (str): Name of the test.\n        trace_dir (str): Directory to store the trace.\n        start_time (datetime): Start time of the trace.\n        end_time (datetime): End time of the trace.\n        steps (list): List of steps in the trace.\n\n    \"\"\"\n\n    def __init__(self, test_name: str, work_dir: str):\n        \"\"\"\n        Initialize the trace handler.\n\n        Args:\n            test_name (str): Name of the test.\n            work_dir (str): Directory to store the trace.\n        \"\"\"\n        self.test_name = test_name\n        self.trace_dir = os.path.join(work_dir, _TRACE_DIR)\n        self.start_time = None\n        self.end_time = None\n        self.steps = []\n\n    def __enter__(self):\n        self.start_time = datetime.now(timezone.utc)\n        return self\n\n    def __exit__(self, *exc):\n        self.end_time = datetime.now(timezone.utc)\n        self._dump_trace()\n\n    def _dump_trace(self):\n        os.makedirs(self.trace_dir, exist_ok=True)\n\n        with open(os.path.join(self.trace_dir, f\"{self.test_name}.json\"), \"w\") as f:\n            json.dump(self._get_trace(), f, default=str)\n\n    def _get_trace(self) -&gt; str:\n        return {\n            \"test_name\": self.test_name,\n            \"start_time\": self.start_time,\n            \"end_time\": self.end_time,\n            \"steps\": self.steps,\n        }\n\n    def add_step(self, step_name: Optional[str] = None, **kwargs):\n        \"\"\"Add a step to the trace.\n\n        Args:\n            step_name (Optional[str]): The name of the step. Defaults to\n                the name of the caller function\n        \"\"\"\n        step_name = step_name or inspect.stack()[1].function\n        step = {\"timestamp\": datetime.now(timezone.utc), \"step_name\": step_name}\n        step.update(kwargs)\n        self.steps.append(step)\n</code></pre>"},{"location":"reference/trace/#src.agenteval.trace.Trace.__init__","title":"<code>__init__(test_name, work_dir)</code>","text":"<p>Initialize the trace handler.</p> <p>Parameters:</p> Name Type Description Default <code>test_name</code> <code>str</code> <p>Name of the test.</p> required <code>work_dir</code> <code>str</code> <p>Directory to store the trace.</p> required Source code in <code>src/agenteval/trace.py</code> <pre><code>def __init__(self, test_name: str, work_dir: str):\n    \"\"\"\n    Initialize the trace handler.\n\n    Args:\n        test_name (str): Name of the test.\n        work_dir (str): Directory to store the trace.\n    \"\"\"\n    self.test_name = test_name\n    self.trace_dir = os.path.join(work_dir, _TRACE_DIR)\n    self.start_time = None\n    self.end_time = None\n    self.steps = []\n</code></pre>"},{"location":"reference/trace/#src.agenteval.trace.Trace.add_step","title":"<code>add_step(step_name=None, **kwargs)</code>","text":"<p>Add a step to the trace.</p> <p>Parameters:</p> Name Type Description Default <code>step_name</code> <code>Optional[str]</code> <p>The name of the step. Defaults to the name of the caller function</p> <code>None</code> Source code in <code>src/agenteval/trace.py</code> <pre><code>def add_step(self, step_name: Optional[str] = None, **kwargs):\n    \"\"\"Add a step to the trace.\n\n    Args:\n        step_name (Optional[str]): The name of the step. Defaults to\n            the name of the caller function\n    \"\"\"\n    step_name = step_name or inspect.stack()[1].function\n    step = {\"timestamp\": datetime.now(timezone.utc), \"step_name\": step_name}\n    step.update(kwargs)\n    self.steps.append(step)\n</code></pre>"},{"location":"targets/","title":"Targets","text":"<p>A target represents the agent you want to test.</p>"},{"location":"targets/#weni-agents-primary-target","title":"\ud83c\udd95 Weni Agents (Primary Target)","text":"<p>The Weni target is the primary focus of this fork, providing native support for testing Weni conversational AI agents.</p>"},{"location":"targets/#quick-start","title":"Quick Start","text":"agenteval.yml<pre><code>target:\n  type: weni\n</code></pre>"},{"location":"targets/#key-features","title":"Key Features","text":"<ul> <li>WebSocket Communication: Real-time bidirectional communication with Weni agents</li> <li>Session Isolation: Each test case uses unique contact identifiers for proper conversation isolation</li> <li>Multi-language Support: Configure language settings for your conversational AI</li> <li>Flexible Authentication: Support for Weni CLI, environment variables, or configuration file</li> </ul> <p> Learn more about Weni Agents</p>"},{"location":"targets/#aws-service-targets","title":"AWS Service Targets","text":"<p>For AWS service targets, additional base configurations apply:</p> <p>AWS Credentials</p> <p>This project uses Boto3's credential resolution chain to determine the AWS credentials to use. Please refer to the Boto3 documentation for more details.</p> agenteval.yml<pre><code>target:\n  aws_profile: my-profile\n  aws_region: us-west-2\n  endpoint_url: my-endpoint-url\n  max_retry: 10\n</code></pre> <p><code>aws_profile</code> (string; optional)</p> <p>A profile name that is used to create a Boto3 session.</p> <p><code>aws_region</code> (string; optional)</p> <p>The AWS region that is used to create a Boto3 session.</p> <p><code>endpoint_url</code> (string; optional)</p> <p>The endpoint URL for the AWS service which is used to construct the Boto3 client.</p> <p><code>max_retry</code> (integer; optional)</p> <p>Configures the Boto3 client with the maximum number of retry attempts allowed. The default is <code>10</code>.</p>"},{"location":"targets/#all-available-targets","title":"All Available Targets","text":""},{"location":"targets/#primary-target","title":"Primary Target","text":"<ul> <li>\ud83c\udd95 Weni Agents - Test Weni conversational AI agents (recommended)</li> </ul>"},{"location":"targets/#aws-service-targets_1","title":"AWS Service Targets","text":"<ul> <li>Agents for Amazon Bedrock</li> <li>Amazon Bedrock Flows</li> <li>Knowledge bases for Amazon Bedrock</li> <li>Amazon Q for Business</li> <li>Amazon SageMaker endpoints</li> <li>Amazon Lex-v2</li> </ul>"},{"location":"targets/#custom-targets","title":"Custom Targets","text":"<ul> <li>Custom Targets - Build your own target implementation</li> </ul>"},{"location":"targets/bedrock_agents/","title":"Agents for Amazon Bedrock","text":"<p>Agents for Amazon Bedrock offers you the ability to build and configure autonomous agents in your application. For more information, visit the AWS documentation here.</p>"},{"location":"targets/bedrock_agents/#prerequisites","title":"Prerequisites","text":"<p>The principal must have the following permissions:</p> <ul> <li>InvokeAgent</li> </ul>"},{"location":"targets/bedrock_agents/#configurations","title":"Configurations","text":"agenteval.yml<pre><code>target:\n  type: bedrock-agent\n  bedrock_agent_id: my-agent-id\n  bedrock_agent_alias_id: my-alias-id\n  bedrock_session_attributes:\n    first_name: user-name\n  bedrock_prompt_session_attributes:\n    timezone: user-timezone\n</code></pre> <p><code>bedrock_agent_id</code> (string)</p> <p>The unique identifier of the Bedrock agent.</p> <p><code>bedrock_agent_alias_id</code> (string)</p> <p>The alias of the Bedrock agent.</p> <p><code>bedrock_session_attributes</code> (map; optional)</p> <p>The attributes that persist over a session between a user and agent, with the same sessionId belong to the same session, as long as the session time limit (the idleSessionTTLinSeconds) has not been surpassed. For example:</p> <pre><code>bedrock_session_attributes:\n  first_name: user-name\n</code></pre> <p><code>bedrock_prompt_session_attributes</code> (map; optional)</p> <p>The attributes that persist over a single call of InvokeAgent. For example:</p> <pre><code>bedrock_prompt_session_attributes:\n    timezone: user-timezone\n</code></pre>"},{"location":"targets/bedrock_flows/","title":"Amazon Bedrock Flows","text":"<p>Amazon Bedrock Flows offer the ability to link prompts, foundation models, and other AWS services into end-to-end workflows through a graphical UI. For more information, visit the AWS documentation here.</p>"},{"location":"targets/bedrock_flows/#prerequisites","title":"Prerequisites","text":"<p>The principal must have the following permissions:</p> <ul> <li>InvokeFlow</li> </ul>"},{"location":"targets/bedrock_flows/#configurations","title":"Configurations","text":"agenteval.yml<pre><code>target:\n  type: bedrock-flow\n  bedrock_flow_id: my-flow-id\n  bedrock_flow_alias_id: my-alias-id\n</code></pre> <p><code>bedrock_flow_id</code> (string)</p> <p>The unique identifier of the Bedrock flow. Typically 10 characters uppercase alphanumeric.</p> <p><code>bedrock_flow_alias_id</code> (string)</p> <p>The alias of the Bedrock flow. Typically 10 characters uppercase alphanumeric.</p>"},{"location":"targets/bedrock_knowledge_bases/","title":"Knowledge bases for Amazon Bedrock","text":"<p>Knowledge bases for Amazon Bedrock provides you the capability of amassing data sources into a repository of information. With knowledge bases, you can easily build an application that takes advantage of retrieval augmented generation (RAG), a technique in which the retrieval of information from data sources augments the generation of model responses. For more information, visit the AWS documentation here.</p>"},{"location":"targets/bedrock_knowledge_bases/#prerequisites","title":"Prerequisites","text":"<p>The principal must have the following permissions:</p> <ul> <li>RetrieveAndGenerate</li> </ul>"},{"location":"targets/bedrock_knowledge_bases/#configurations","title":"Configurations","text":"agenteval.yml<pre><code>target:\n  type: bedrock-knowledge-base\n  model_id: my-model-id\n  knowledge_base_id: my-kb-id\n</code></pre> <p><code>model_id</code> (string)</p> <p>The unique identifier of the foundation model used to generate a response.</p> <p><code>knowledge_base_id</code> (string)</p> <p>The unique identifier of the knowledge base that is queried and the foundation model used for generation.</p>"},{"location":"targets/custom_targets/","title":"Custom Targets","text":"<p>If you want to test an agent that is not natively supported, you can bring your own Target by defining a Python module containing a subclass of BaseTarget. The name of this module must contain the suffix <code>_target</code> (e.g. <code>my_custom_target</code>).</p> <p>The subclass should implement the <code>invoke</code> method to invoke your agent and return a TargetResponse.</p> my_custom_target.py<pre><code>from agenteval.targets import BaseTarget, TargetResponse\nfrom my_agent import MyAgent\n\nclass MyCustomTarget(BaseTarget):\n\n    def __init__(self, **kwargs):\n        self.agent = MyAgent(**kwargs)\n\n    def invoke(self, prompt: str) -&gt; TargetResponse:\n\n        response = self.agent.invoke(prompt)\n\n        return TargetResponse(response=response)\n</code></pre> <p>Once you have created your subclass, specify the module path to the Target.</p> agenteval.yml<pre><code>target:\n  type: path.to.my_custom_target.MyCustomTarget`\n  my_agent_parameter: \"value\" # (1)\n</code></pre> <ol> <li>This will be passed as <code>kwargs</code> when initializing the Target.</li> </ol> <p>Warning</p> <p>During a run, an instance of the Target will be created for each test in the test plan. We recommend avoiding testing Targets that load large models or vector stores into memory, as this can lead to a memory error. Consider deploying your agent and exposing it as a RESTful service.</p>"},{"location":"targets/custom_targets/#examples","title":"Examples","text":""},{"location":"targets/custom_targets/#rest-api","title":"REST API","text":"<p>We will implement a custom Target that invokes an agent exposed as a REST API.</p> <p>my_api_target.py</p> <pre><code>import json\n\nimport requests\n\nfrom agenteval.targets import BaseTarget, TargetResponse\n\n\nclass MyAPITarget(BaseTarget):\n    def __init__(self, **kwargs):\n        self.url = kwargs.get(\"url\")\n\n    def invoke(self, prompt: str) -&gt; TargetResponse:\n        data = {\"message\": prompt}\n\n        response = requests.post(\n            self.url, json=data, headers={\"Content-Type\": \"application/json\"}\n        )\n\n        return TargetResponse(response=json.loads(response.content)[\"agentResponse\"])\n</code></pre> <p>Create a test plan that references <code>MyAPITarget</code>.</p> <p>agenteval.yml</p> <pre><code>evaluator:\n  model: claude-3\ntarget:\n  type: my_api_target.MyAPITarget\n  url: https://api.example.com/invoke\ntests:\n  get_backlog_tickets:\n    steps:\n    - Ask agent how many tickets are left in the backlog\n    expected_results:\n    - Agent responds with 15 tickets\n</code></pre>"},{"location":"targets/custom_targets/#langchain-agent","title":"LangChain agent","text":"<p>We will create a simple LangChain agent which calculates the length of a given piece of text.</p> <p>my_langchain_target.py</p> <pre><code>from langchain_community.llms import Bedrock\nfrom langchain.agents import tool\nfrom langchain import hub\nfrom langchain.agents import AgentExecutor, create_xml_agent\n\nfrom agenteval.targets import BaseTarget, TargetResponse\n\nllm = Bedrock(model_id=\"anthropic.claude-v2:1\")\n\n@tool\ndef calculate_text_length(text: str) -&gt; int:\n    \"\"\"Returns the length of a given text.\"\"\"\n    return len(text)\n\n\ntools = [calculate_text_length]\nprompt = hub.pull(\"hwchase17/xml-agent-convo\")\n\nagent = create_xml_agent(llm, tools, prompt)\n\n\nclass MyLangChainTarget(BaseTarget):\n    def __init__(self, **kwargs):\n        self.agent = AgentExecutor(agent=agent, tools=tools, verbose=False)\n\n    def invoke(self, prompt: str) -&gt; TargetResponse:\n\n        response = self.agent.invoke({\"input\": prompt})[\"output\"]\n\n        return TargetResponse(response=response)\n</code></pre> <p>Create a test plan that references <code>MyLangChainTarget</code>.</p> <p>agenteval.yml</p> <pre><code>evaluator:\n  model: claude-3\ntarget:\n  type: my_langchain_target.MyLangChainTarget\ntests:\n  calculate_text_length:\n    steps:\n    - \"Ask agent to calculate the length of this text: Hello world!\"\n    expected_results:\n    - The agent responds that the length is 12.\n</code></pre>"},{"location":"targets/lex_v2/","title":"Amazon Lex v2","text":"<p>Amazon Lex V2 is an AWS service for building conversational interfaces for applications using voice and text. Amazon Lex V2 provides the deep functionality and flexibility of natural language understanding (NLU) and automatic speech recognition (ASR) so you can build highly engaging user experiences with lifelike, conversational interactions, and create new categories of products. For more information, visit the AWS documentation here.</p>"},{"location":"targets/lex_v2/#prerequisites","title":"Prerequisites","text":"<p>The principal must have the following permissions:</p> <ul> <li>RecognizeText</li> </ul>"},{"location":"targets/lex_v2/#configurations","title":"Configurations","text":"agenteval.yml<pre><code>target:\n  type: lex-v2\n  bot_id: my-bot-id\n  bot_alias_id: my-bot-alias-id\n  locale_id: my-locale-id\n</code></pre> <p><code>bot_id</code> (string)</p> <p>The unique identifier of the Amazon Lex v2 chatbot.</p> <p><code>bot_alias_id</code> (string; optional)</p> <p>The alias of the Amazon Lex v2 chatbot.</p> <p><code>locale_id</code> (string; optional)</p> <p>The locale of the Amazon Lex v2 chatbot.</p>"},{"location":"targets/q_business/","title":"Amazon Q Business","text":"<p>Amazon Q Business is a generative AI\u2013powered assistant that can answer questions, provide summaries, generate content, and securely complete tasks based on data and information in your enterprise systems. For more information, visit the AWS documentation here.</p>"},{"location":"targets/q_business/#prerequisites","title":"Prerequisites","text":"<p>The principal must have the following permissions:</p> <ul> <li>ChatSync</li> </ul>"},{"location":"targets/q_business/#configurations","title":"Configurations","text":"agenteval.yml<pre><code>target:\n  type: q-business\n  q_business_application_id: my-app-id\n  q_business_user_id: my-user-id\n</code></pre> <p><code>q_business_application_id</code> (string)</p> <p>The unique identifier of the Amazon Q application.</p> <p><code>q_business_user_id</code> (string; optional)</p> <p>The identifier of the Amazon Q user.</p>"},{"location":"targets/sagemaker_endpoints/","title":"Amazon SageMaker endpoints","text":"<p>Amazon SageMaker is a fully managed service to prepare data and build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows. SageMaker endpoints are used to host ML models for real-time inference use cases. For more information, visit the AWS documentation here.</p> <p>Info</p> <p>The current implementation uses JSON to encode the data sent in requests and decode the data received in responses. Please ensure that your SageMaker endpoint can handle requests and responses with the Content-Type and Accept headers set to <code>application/json</code> before proceeding.</p>"},{"location":"targets/sagemaker_endpoints/#prerequisites","title":"Prerequisites","text":"<p>The principal must have the following permissions:</p> <ul> <li>InvokeEndpoint</li> </ul>"},{"location":"targets/sagemaker_endpoints/#configurations","title":"Configurations","text":"agenteval.yml<pre><code>target:\n  type: sagemaker-endpoint\n  endpoint_name: my-endpoint-name\n  request_body:\n    input_text: None\n    temperature: 0.1\n  input_path: $.input_text\n  output_path: $.[0].generated_text\n  custom_attributes: my-attributes\n  target_model: my-model\n  target_variant: my-variant\n  target_container_hostname: my-hostname\n  inference_component_name: my-component-name\n</code></pre> <p><code>endpoint_name</code> (string)</p> <p>The name of the Amazon SageMaker endpoint.</p> <p><code>request_body</code> (map)</p> <p>The data that is sent to the endpoint, which includes a placeholder for the prompt. During a run, the placeholder will be replaced by a prompt generated by the Evaluator. For example:</p> <pre><code>request_body:\n  input_text: None # prompt\n  temperature: 0.1\n</code></pre> <p><code>input_path</code> (string)</p> <p>A JSONPath expression to match the field for the input prompt in the request body. For the <code>request_body</code> below:</p> <pre><code>request_body:\n  input_text: None # prompt\n  temperature: 0.1\n</code></pre> <p>The <code>input_path</code> would be <code>$.input_text</code>.</p> <p><code>output_path</code> (string)</p> <p>A JSONPath expression to match the generated text in the response body. For example, if the endpoint returns the following:</p> <pre><code>[{ \"generated_text\": \"Hello!\" }]\n</code></pre> <p>The <code>output_path</code> would be <code>$.[0].generated_text</code>.</p> <p><code>custom_attributes</code> (string; optional)</p> <p>Provides additional information about a request for an inference submitted to a model hosted at an Amazon SageMaker endpoint.</p> <p><code>target_model</code> (string; optional)</p> <p>The model to request for inference when invoking a multi-model endpoint.</p> <p><code>target_variant</code> (string; optional)</p> <p>The production variant to send the inference request to when invoking an endpoint that is running two or more variants.</p> <p><code>target_container_hostname</code> (string; optional)</p> <p>The hostname of the container to invoke if the endpoint hosts multiple containers and is configured to use direct invocation.</p> <p><code>inference_component_name</code> (string; optional)</p> <p>The name of the inference component to invoke if the endpoint hosts one or more inference components.</p>"},{"location":"targets/weni/","title":"Weni Agents","text":"<p>Weni is an intelligent agent platform that enables conversational AI experiences. This target allows you to test Weni agents through their API and WebSocket interface.</p>"},{"location":"targets/weni/#prerequisites","title":"Prerequisites","text":"<p>Important</p> <p>You need both AWS and Weni credentials to run evaluations!</p> <p>To use the Weni target, you need:</p> <ol> <li>AWS Credentials: Required for the evaluator (Claude model via Bedrock)</li> <li>AWS Access Key ID</li> <li>AWS Secret Access Key</li> <li> <p>AWS Session Token</p> </li> <li> <p>A Weni Project: An active project in the Weni platform</p> </li> <li> <p>Weni Authentication: Choose one of the following methods:</p> </li> </ol> <p>\ud83d\ude80 Option 1: Weni CLI (Recommended)</p> <p>Install and authenticate with the Weni CLI:    <pre><code># Install Weni CLI\npip install weni-cli\n\n# Authenticate with Weni\nweni login\n\n# Select your project\nweni project use [your-project-uuid]\n</code></pre></p> <p>Get the Weni CLI from: https://github.com/weni-ai/weni-cli</p> <p>\ud83d\udccb Option 2: Environment Variables</p> <p>Set these environment variables manually:    - <code>WENI_PROJECT_UUID</code>: Your project's unique identifier    - <code>WENI_BEARER_TOKEN</code>: Your authentication bearer token</p> <p>\u2699\ufe0f Option 3: Configuration File</p> <p>Provide credentials directly in your test configuration file.</p>"},{"location":"targets/weni/#installation","title":"Installation","text":"<p>The Weni target is included with the main package:</p> <pre><code>pip install weni-agenteval\n</code></pre> <p>The required <code>websocket-client</code> package is automatically installed as a dependency.</p>"},{"location":"targets/weni/#configuration","title":"Configuration","text":"agenteval.yml<pre><code>target:\n  type: weni\n  # language: pt-BR  # Optional, defaults to pt-BR\n  # timeout: 30      # Optional, max seconds to wait for response\n  # Optionally provide credentials directly (not recommended):\n  # weni_project_uuid: your-project-uuid\n  # weni_bearer_token: your-bearer-token\n</code></pre>"},{"location":"targets/weni/#parameters","title":"Parameters","text":"<p><code>weni_project_uuid</code> (string; optional)</p> <p>The unique identifier of your Weni project. If not provided, the target will look for the <code>WENI_PROJECT_UUID</code> environment variable.</p> <pre><code>weni_project_uuid: 45718786-4066-4338-9e86-f3ea525224d2\n</code></pre> <p><code>weni_bearer_token</code> (string; optional)</p> <p>The bearer token for authenticating with the Weni API. If not provided, the target will look for the <code>WENI_BEARER_TOKEN</code> environment variable.</p> <pre><code>weni_bearer_token: your-bearer-token-here\n</code></pre> <p><code>language</code> (string; optional)</p> <p>The language code for the conversation. This affects how the agent processes and responds to messages. Defaults to <code>\"pt-BR\"</code>.</p> <p>Common language codes: - <code>pt-BR</code>: Brazilian Portuguese (default) - <code>en-US</code>: American English - <code>es</code>: Spanish - <code>fr</code>: French</p> <pre><code>language: en-US\n</code></pre> <p><code>timeout</code> (integer; optional)</p> <p>Maximum time in seconds to wait for the agent's response via WebSocket. Defaults to <code>30</code>.</p> <pre><code>timeout: 45\n</code></pre>"},{"location":"targets/weni/#how-it-works","title":"How It Works","text":"<p>The Weni target interacts with Weni agents through a two-step process:</p> <ol> <li>Message Sending: Sends the user prompt to the Weni API via HTTP POST request</li> <li>Response Reception: Connects to a WebSocket endpoint to receive the agent's asynchronous response</li> </ol> <p>Each test case uses a unique contact identifier (<code>contact_urn</code>) to maintain conversation isolation and history throughout the test session.</p>"},{"location":"targets/weni/#example-test-plan","title":"Example Test Plan","text":"weni_test_plan.yml<pre><code>evaluator:\n  model: claude-3\n\ntarget:\n  type: weni\n\ntests:\n  greeting:\n    steps:\n      - Send a greeting \"Ol\u00e1, bom dia!\"\n    expected_results:\n      - Agent responds with a friendly greeting\n      - Agent introduces itself or explains its capabilities\n\n  product_inquiry:\n    steps:\n      - Ask \"Quais produtos voc\u00eas t\u00eam dispon\u00edveis?\"\n    expected_results:\n      - Agent provides information about available products\n      - Response includes clear product descriptions or categories\n\n  multi_turn_conversation:\n    steps:\n      - Ask \"Qual \u00e9 o hor\u00e1rio de funcionamento?\"\n      - Follow up with \"E aos finais de semana?\"\n    expected_results:\n      - Agent provides business hours for weekdays\n      - Agent maintains context and provides weekend hours\n      - Responses are coherent and contextual\n\n  help_request:\n    steps:\n      - Say \"Preciso de ajuda com um problema\"\n    expected_results:\n      - Agent offers assistance\n      - Agent asks clarifying questions about the problem\n      - Response is empathetic and helpful\n\n  error_handling:\n    steps:\n      - Send an unclear message \"xyz123 !!!\"\n    expected_results:\n      - Agent handles the unclear input gracefully\n      - Agent asks for clarification or provides guidance\n      - No error messages are exposed to the user\n</code></pre>"},{"location":"targets/weni/#running-tests","title":"Running Tests","text":""},{"location":"targets/weni/#using-weni-cli-recommended","title":"Using Weni CLI (Recommended)","text":"<p>If you're using the Weni CLI for authentication:</p> <pre><code>weni-agenteval run\n</code></pre> <p>The tool automatically looks for <code>agenteval.yml</code> in the current directory.</p>"},{"location":"targets/weni/#using-environment-variables","title":"Using Environment Variables","text":"<pre><code>export WENI_PROJECT_UUID=\"your-project-uuid\"\nexport WENI_BEARER_TOKEN=\"your-bearer-token\"\nweni-agenteval run\n</code></pre>"},{"location":"targets/weni/#additional-cli-options","title":"Additional CLI Options","text":"<pre><code># Run with verbose output\nweni-agenteval run --verbose\n\n# Run specific tests only\nweni-agenteval run --filter \"greeting,purchase_outside_postal_code\"\n\n# Run from a different directory\nweni-agenteval run --plan-dir /path/to/test/directory\n\n# Initialize a new test plan template\nweni-agenteval init\n</code></pre>"},{"location":"targets/weni/#troubleshooting","title":"Troubleshooting","text":""},{"location":"targets/weni/#common-issues","title":"Common Issues","text":"<p>AWS Authentication Errors - Verify your AWS environment variables are set correctly (ACCESS_KEY_ID, SECRET_ACCESS_KEY, SESSION_TOKEN) - Ensure you have access to Amazon Bedrock in your specified region - Check that your AWS credentials have the necessary Bedrock permissions - Verify the <code>aws_region</code> in your configuration matches your AWS account's region access</p> <p>Weni Authentication Errors - Using Weni CLI (Recommended): Run <code>weni login</code> to re-authenticate, then <code>weni project use [project-uuid]</code> to select your project - Using Environment Variables: Verify your <code>WENI_BEARER_TOKEN</code> is valid and not expired - Check that the <code>WENI_PROJECT_UUID</code> matches your actual project - Ensure the bearer token has the necessary permissions for the project - Get Weni CLI at: https://github.com/weni-ai/weni-cli</p> <p>Connection Issues - Verify the Weni API endpoints are accessible from your network - Check for any firewall or proxy settings blocking HTTPS/WSS connections - Ensure your internet connection is stable</p> <p>Timeout Errors - Increase the <code>timeout</code> parameter if your agent requires more processing time - Check if the agent is properly configured and active in the Weni platform - Verify the agent is not stuck in a processing loop</p> <p>WebSocket Connection Failures - Ensure the <code>websocket-client</code> package is properly installed - Check for any proxy configurations that might interfere with WebSocket connections - Verify the WebSocket endpoint URL is correct for your project</p>"},{"location":"targets/weni/#debug-logging","title":"Debug Logging","text":"<p>To enable detailed logging for troubleshooting:</p> <pre><code>import logging\nlogging.basicConfig(level=logging.DEBUG)\n</code></pre> <p>This will show detailed information about: - API request/response details - WebSocket connection status - Message processing steps - Error details</p>"},{"location":"targets/weni/#limitations","title":"Limitations","text":"<ul> <li>Each test case maintains its own conversation context through a unique contact identifier</li> <li>The target waits for the <code>finalResponse</code> message type and ignores intermediate processing messages</li> <li>Response time depends on the agent's complexity and the Weni platform's processing time</li> </ul>"},{"location":"targets/weni/#see-also","title":"See Also","text":"<ul> <li>Custom Targets - Learn how to create your own custom targets</li> <li>Configuration - General configuration options</li> <li>User Guide - Complete guide to using Agent Evaluation</li> </ul>"}]}